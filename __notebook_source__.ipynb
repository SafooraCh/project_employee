# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# ==============================================================================
# EMPLOYEE PRODUCTIVITY ANALYSIS - KAGGLE VERSION
# ==============================================================================

# STEP 1: IMPORT LIBRARIES
# ==============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score
import pickle

# ==============================================================================
# STEP 2: LOAD DATASET
# ==============================================================================
df = pd.read_csv('/kaggle/input/employee-productivity/employee_productivity.csv')

print(f"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns")

# ==============================================================================
# STEP 3: BASIC DATA EXPLORATION
# ==============================================================================
print("First 5 rows:")
display(df.head())

print("\nDataset info:")
df.info()

print("\nStatistical summary:")
display(df.describe())

# Check for missing values
print("\nMissing values:")
missing = df.isnull().sum()
display(missing[missing > 0])

# Missing value heatmap
plt.figure(figsize=(10, 4))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

# ==============================================================================
# STEP 4: IDENTIFY COLUMN TYPES
# ==============================================================================
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

print(f"\nNumerical columns: {numerical_cols}")
print(f"Categorical columns: {categorical_cols}")

# ==============================================================================
# STEP 5: VISUALIZE NUMERICAL DATA
# ==============================================================================
for col in numerical_cols[:3]:
    plt.figure(figsize=(10, 4))
    plt.subplot(1, 2, 1)
    df[col].hist(bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Distribution of {col}')
    plt.subplot(1, 2, 2)
    df.boxplot(column=col)
    plt.title(f'Boxplot of {col}')
    plt.tight_layout()
    plt.show()

# ==============================================================================
# STEP 6: VISUALIZE CATEGORICAL DATA
# ==============================================================================
for col in categorical_cols[:2]:
    print(f"{col} value counts:")
    display(df[col].value_counts())
    plt.figure(figsize=(10, 4))
    df[col].value_counts().plot(kind='bar', color='coral')
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45)
    plt.show()

# ==============================================================================
# STEP 7: CORRELATION ANALYSIS
# ==============================================================================
correlation = df[numerical_cols].corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

# ==============================================================================
# STEP 8: DATA PREPROCESSING
# ==============================================================================
df_clean = df.copy()

# Fill missing values
for col in df_clean.columns:
    if df_clean[col].isnull().sum() > 0:
        if df_clean[col].dtype in ['int64', 'float64']:
            df_clean[col].fillna(df_clean[col].median(), inplace=True)
        else:
            df_clean[col].fillna(df_clean[col].mode()[0], inplace=True)

# Encode categorical variables
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_clean[col + '_encoded'] = le.fit_transform(df_clean[col].astype(str))
    label_encoders[col] = le

# Scale numerical columns
scaler = StandardScaler()
df_clean[numerical_cols] = scaler.fit_transform(df_clean[numerical_cols])

# ==============================================================================
# STEP 9: PREPARE DATA FOR ML
# ==============================================================================
target_col = df_clean.columns[-1]
unique_values = df_clean[target_col].nunique()
is_classification = unique_values <= 10

X = df_clean.select_dtypes(include=['int64', 'float64']).drop(columns=[target_col], errors='ignore')
y = df_clean[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ==============================================================================
# STEP 10: TRAIN MODEL
# ==============================================================================
if is_classification:
    model = RandomForestClassifier(n_estimators=100, random_state=42)
else:
    model = RandomForestRegressor(n_estimators=100, random_state=42)

model.fit(X_train, y_train)
y_pred = model.predict(X_test)

if is_classification:
    print(f"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%")
    print(classification_report(y_test, y_pred))
else:
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    print(f"RMSE: {rmse:.4f}, R2: {r2*100:.2f}%")
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('Actual')
    plt.ylabel('Predicted')
    plt.title('Actual vs Predicted')
    plt.show()

# ==============================================================================
# STEP 11: FEATURE IMPORTANCE
# ==============================================================================
importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
plt.barh(importance['Feature'].head(10), importance['Importance'].head(10))
plt.gca().invert_yaxis()
plt.xlabel('Importance')
plt.title('Top 10 Features')
plt.show()

# ==============================================================================
# STEP 12: SAVE MODEL AND PREPROCESSORS
# ==============================================================================
with open('/kaggle/working/best_model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('/kaggle/working/scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

with open('/kaggle/working/label_encoders.pkl', 'wb') as f:
    pickle.dump(label_encoders, f)

print("âœ… Model, scaler, and encoders saved in /kaggle/working/")

